# Tracez Testing Suite: Verification by Demonstration

This directory contains the comprehensive testing approach for tracez, organized around the principle: **If we can't prove it, we don't claim it.**

## Testing Philosophy

Every performance claim, reliability assertion, and behavior guarantee in tracez is backed by executable tests. This isn't just code coverage - it's claim validation.

### The Verification Standard

- **Performance claims** → Benchmarks with realistic loads
- **Reliability claims** → Chaos testing and failure injection  
- **Behavior claims** → Integration tests with real patterns
- **Documentation claims** → Runnable examples that prove statements

## Testing Structure

### [Benchmarks](benchmarks/) - Performance Reality Check
**Purpose:** Validate performance claims under production conditions  
**Scope:** Memory usage, CPU efficiency, throughput, scalability  
**Key Tests:**
- Single-threaded: ~1.58M spans/sec verified
- Parallel scaling: 3x performance improvement measured  
- Memory efficiency: 312 B/op, 8 allocs/op documented
- Optimization impact: ID pools, context bundling, buffer management

**Run:** `cd benchmarks && ./run_benchmarks.sh`

### [Integration](integration/) - Real-World Pattern Validation  
**Purpose:** Test tracez behavior in realistic distributed system scenarios  
**Scope:** Cross-cutting concerns, failure modes, edge cases  
**Categories:**
- **API Patterns**: Gateway routing, rate limiting, GraphQL, WebSocket
- **Service Mesh**: Circuit breakers, saga patterns, event sourcing
- **Database Patterns**: Transactions, connection pooling, N+1 detection
- **Observability**: Sampling, aggregation, correlation, percentiles
- **Reliability**: Memory pressure, panic recovery, graceful degradation

**Run:** `cd integration && go test -v`

## Testing Approach

### 1. Proof by Measurement
Don't assert performance - measure it:
```go
// WRONG: "Tracez has low overhead"
// RIGHT: BenchmarkSpanCreation shows 632 ns/op
```

### 2. Failure-First Testing
Test what breaks, not just what works:
- Collector backpressure under sustained load
- Context cancellation during span creation
- Memory exhaustion scenarios
- Concurrent collector access patterns

### 3. Realistic Scenarios  
Test patterns that actually occur in production:
- HTTP middleware chains with 5+ components
- Database transaction patterns with nested operations
- Worker pool patterns with mixed workload types
- Service mesh communication with retries and timeouts

### 4. Integration Over Isolation
Test component interactions, not just individual functions:
- Cross-goroutine context propagation
- Collector competition under high concurrency
- Buffer growth patterns during traffic spikes
- Parent-child span relationships in complex trees

## Key Testing Patterns

### MockCollector Pattern
Synchronous collection for deterministic testing:
```go
collector := NewMockCollector(t, "test", 1000)
// Enables predictable span ordering and verification
```

### Load Testing Pattern  
Realistic concurrent load generation:
```go
// Simulate 100 concurrent workers for 10 seconds
// Measure actual throughput, not theoretical capacity
```

### Chaos Testing Pattern
Inject realistic failures during operation:
```go
// Cancel contexts mid-trace
// Exhaust collector buffers
// Trigger memory pressure
// Validate graceful degradation
```

## Running Tests

### Full Suite
```bash
# All integration tests
make test-integration

# All benchmarks with comparison
make benchmark

# Specific test categories
go test -v ./testing/integration -run TestServiceMesh
go test -bench=BenchmarkMemory ./testing/benchmarks
```

### Continuous Integration  
Tests run automatically on:
- Every commit (basic suite)
- Pull requests (full integration)
- Release builds (extended benchmarks)
- Performance regression detection

## Test Coverage Philosophy

We don't chase percentage coverage - we chase **behavior coverage**:

- ✓ **Claim coverage**: Every documented behavior has a test
- ✓ **Failure coverage**: Every failure mode has a reproduction test
- ✓ **Performance coverage**: Every optimization has a benchmark
- ✓ **Integration coverage**: Every component interaction has a validation test

### What We Don't Test
- Internal implementation details that don't affect behavior
- Error messages (unless they guide troubleshooting)  
- Private function logic (test through public interfaces)
- Theoretical edge cases that don't occur in practice

## The Reality Check

These tests serve as the ground truth for tracez behavior:

- **Documentation claims** are verified by runnable examples
- **Performance numbers** are generated by benchmark measurements  
- **Reliability assertions** are proven by chaos testing
- **Integration behavior** is validated by realistic scenarios

If a test fails, either:
1. The code has a bug (fix the code)
2. The test has wrong assumptions (fix the test)  
3. The documentation makes false claims (fix the docs)

Never "fix" tests by making them pass false assertions.

## Contributing to Testing

When adding features:
1. **Add integration test** - How does this interact with existing components?
2. **Add benchmark** - What's the performance impact?
3. **Add failure test** - How does this degrade under stress?
4. **Update documentation** - What new behavior needs verification?

The testing suite is the contract between what we promise and what we deliver. It's not overhead - it's the foundation of trust.